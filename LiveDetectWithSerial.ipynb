{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import time\n",
    "import serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'jpeg' instead of 'png' (~5 times faster)\n",
    "def array_to_image(a, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    #Convert array to binary stream object\n",
    "    Image.fromarray(a).save(f, fmt)\n",
    "    \n",
    "    return IPython.display.Image(data=f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(cam):\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    # Natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_locations(image):\n",
    "    return face_recognition.face_locations(image)\n",
    "\n",
    "def draw_box_on_face(image, face_location):\n",
    "    top, right, bottom, left = face_location\n",
    "#     w, h, _ = image.shape\n",
    "#     border = (w + h / 2) / 16\n",
    "    border = 8\n",
    "    cv2.rectangle(image, (left - border, top - border), (right + border // 2, bottom + border // 2), (0, 255, 0), border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_webcam(display, image):\n",
    "    frame = array_to_image(image)   \n",
    "    display.update(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_emotion_detection(image, face_location):\n",
    "    top, right, bottom, left = face_location\n",
    "    \n",
    "    face_image = image[top:bottom, left:right]\n",
    "    face_image = cv2.resize(face_image, (48, 48))\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    return np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(img):\n",
    "    emotion_dict = {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}\n",
    "    label_map = dict((v,k) for k,v in emotion_dict.items()) \n",
    "    \n",
    "    predicted_class = np.argmax(model.predict(img))\n",
    "    return label_map[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_arduino(emotion):\n",
    "    serial_dict = { 'Happy': b'1', 'Sad': b'2', 'Fear': b'3', 'Disgust': b'4', 'Angry': b'5', 'Neutral': b'6', 'Surprise': b'7', }\n",
    "    ser.write(serial_dict[emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "d = IPython.display.display(\"\", display_id=1)\n",
    "d2 = IPython.display.display(\"\", display_id=2)\n",
    "d3 = IPython.display.display(\"\", display_id=3)\n",
    "\n",
    "model = load_model(\"face_and_emotion_detection/emotion_detector_models/model_v6_23.hdf5\")\n",
    "ser = serial.Serial('/dev/cu.usbmodem142101', 9600)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        \n",
    "        image = get_frame(cam)\n",
    "        face_locations = get_face_locations(image)\n",
    "        \n",
    "        if len(face_locations) < 1:\n",
    "            update_webcam(d, image)\n",
    "            continue\n",
    "            \n",
    "        face_location = face_locations[0]\n",
    "        draw_box_on_face(image, face_location)\n",
    "        update_webcam(d, image)\n",
    "\n",
    "        detectable_img = prep_for_emotion_detection(image, face_location)\n",
    "        emotion = detect_emotion(detectable_img)\n",
    "        d2.update(IPython.display.HTML(emotion))\n",
    "        send_to_arduino(emotion)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        s = f\"\"\"{int(1/(t2-t1))} FPS\"\"\"\n",
    "        d3.update(IPython.display.HTML(s))\n",
    "    except KeyboardInterrupt:\n",
    "        print()\n",
    "        cam.release()\n",
    "        IPython.display.clear_output()\n",
    "        print (\"Stream stopped\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
